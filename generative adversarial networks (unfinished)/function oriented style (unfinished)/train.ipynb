{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training is done in this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipynb.fs.full.network import *\n",
    "from ipynb.fs.full.helper import *\n",
    "from ipynb.fs.full.cifar_10_parser import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if fetching data from google drive then change the address accordingly\n",
    "\n",
    "folder_path = r'C:\\Users\\Admin\\Desktop\\program\\datasets\\cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for the input images(discriminator) and noise(generator)\n",
    "\n",
    "input_image = tf.placeholder(tf.float32, [None, 32,32,3]) # feed input images\n",
    "gen_image = tf.placeholder(tf.float32, [None, 1,1,100]) # feed random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables for generator and discriminator\n",
    "\n",
    "gen = generator(gen_image)\n",
    "dis_output_real, dis_logits_real = discriminator(input_image)\n",
    "dis_output_fake, dis_logits_fake = discriminator(gen, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the trainable variables to apply regularization\n",
    "\n",
    "vars_g = [var for var in tf.trainable_variables() if var.name.startswith('g')]\n",
    "vars_d = [var for var in tf.trainable_variables() if var.name.startswith('d')]\n",
    "\n",
    "reg_g = tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(1e-6), vars_g)\n",
    "reg_d = tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(1e-6), vars_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_loss_real = helper_loss(tf.ones_like(dis_logits_real), dis_logits_real)\n",
    "dis_loss_fake = helper_loss(tf.zeros_like(dis_logits_fake), dis_logits_fake)\n",
    "\n",
    "dis_loss = tf.reduce_mean((dis_loss_real + dis_loss_fake)*0.5)\n",
    "gen_loss = tf.reduce_mean(helper_loss(tf.ones_like(dis_logits_fake), dis_logits_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer_dis = tf.train.AdamOptimizer(learning_rate=0.0002).minimize(dis_loss + reg_d, var_list = vars_d)\n",
    "    optimizer_gen = tf.train.AdamOptimizer(learning_rate=0.0002).minimize(dis_loss + reg_d, var_list = vars_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training takes a lot of time so create and run the session on cloud gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(60000):\\n    train_d = True\\n    train_g = True\\n    batch_size = 50\\n    # keep_prob_train = 0.6 \\n    \\n    \\n    n = np.random.uniform(0.0, 1.0, [batch_size, 1,1,100]).astype(np.float32)   \\n    # batch = [np.reshape(b, [28, 28]) for b in mnist.train.next_batch(batch_size=batch_size)[0]]  \\n    \\n    d_real_ls, d_fake_ls, g_ls, d_ls = sess.run([loss_d_real, loss_d_fake, loss_g, loss_d], \\n                                                feed_dict={X_in: batch, \\n                                                           noise: n})\\n    \\n    d_real_ls = np.mean(d_real_ls)\\n    d_fake_ls = np.mean(d_fake_ls)\\n    g_ls = g_ls\\n    d_ls = d_ls\\n    \\n    if g_ls * 1.5 < d_ls:\\n        train_g = False\\n        pass\\n    if d_ls * 2 < g_ls:\\n        train_d = False\\n        pass\\n    \\n    if train_d:\\n        sess.run(optimizer_d, feed_dict={noise: n, X_in: batch})\\n        \\n        \\n    if train_g:\\n        sess.run(optimizer_g, feed_dict={gen_image: n})\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in range(60000):\n",
    "    train_d = True\n",
    "    train_g = True\n",
    "    batch_size = 50\n",
    "    # keep_prob_train = 0.6 \n",
    "    \n",
    "    \n",
    "    n = np.random.uniform(0.0, 1.0, [batch_size, 1,1,100]).astype(np.float32)   \n",
    "    # batch = [np.reshape(b, [28, 28]) for b in mnist.train.next_batch(batch_size=batch_size)[0]]  \n",
    "    \n",
    "    d_real_ls, d_fake_ls, g_ls, d_ls = sess.run([loss_d_real, loss_d_fake, loss_g, loss_d], \n",
    "                                                feed_dict={X_in: batch, \n",
    "                                                           noise: n})\n",
    "    \n",
    "    d_real_ls = np.mean(d_real_ls)\n",
    "    d_fake_ls = np.mean(d_fake_ls)\n",
    "    g_ls = g_ls\n",
    "    d_ls = d_ls\n",
    "    \n",
    "    if g_ls * 1.5 < d_ls:\n",
    "        train_g = False\n",
    "        pass\n",
    "    if d_ls * 2 < g_ls:\n",
    "        train_d = False\n",
    "        pass\n",
    "    \n",
    "    if train_d:\n",
    "        sess.run(optimizer_d, feed_dict={noise: n, X_in: batch})\n",
    "        \n",
    "        \n",
    "    if train_g:\n",
    "        sess.run(optimizer_g, feed_dict={gen_image: n})\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith tf.Session() as sess:\\n    \\n    batch_size = 50\\n    \\n    num_epochs = 25\\n    num_batches = 200*6\\n    \\n    # input for the generator\\n    n = np.random.uniform(0.0, 1.0, [batch_size, 1,1,100]).astype(np.float32)\\n    #batch_iter = batch_generator(folder_path, batch_size)\\n    \\n    for epoch in range(num_epochs):\\n        \\n        # refresh the iterator after each epoch\\n        batch_iter = batch_generator(folder_path, batch_size)\\n        new_batch_flag = 1\\n        batch_counter = 0 # counts number of new batches used\\n        \\n            #for i in range(num_batches):\\n            while batch_counter < num_batches:\\n                train_d = True\\n                train_g = True\\n                \\n                \\n                # will be zero if the previous batch was not used to train the discriminator\\n                # so that the next batch will not be generated (the old batch will be reused)\\n                # problem : the loop will run 1200 times but 1200 new batches wont be used\\n                # possible solution : create a counter(for num of new batches used) \\n                #     and use while loop inplace of the inner for loop\\n                \\n                if new_batch_flag == 1: \\n                    batch = next(batch_iter) # generate the next batch out of 200*6 batches\\n                    batch_counter += 1\\n                    \\n                d_real_ls, d_fake_ls, g_ls, d_ls = sess.run([dis_loss_real, dis_loss_fake, gen_loss, dis_loss], \\n                                                            feed_dict={input_image: batch, gen_image: n})\\n                \\n                d_real_ls = np.mean(d_real_ls)\\n                d_fake_ls = np.mean(d_fake_ls)\\n                g_ls = g_ls\\n                d_ls = d_ls\\n                \\n                \\n                # the code below will create a problem\\n                # because the next batch will be generated but the \\n                # discriminator will not be trained if train_d = False\\n                # so we are losing some of the training data\\n                \\n                # possible solution : generate a batch only \"if train_d = True\"\\n                # or remove the constraints on training\\n                # or create a flag as a feed back for whether or not the batch was used in the previous iteration\\n                \\n                if g_ls * 1.5 < d_ls:\\n                    train_g = False\\n                    pass\\n                if d_ls * 2 < g_ls:\\n                    train_d = False\\n                    new_batch_flag = 0\\n                    pass\\n                if train_d:\\n                    sess.run(optimizer_d, feed_dict={gen_image: n, input_image: batch})\\n                    new_batch_flag = 1\\n\\n\\n                if train_g:\\n                    sess.run(optimizer_g, feed_dict={gen_image: n})\\n                \\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#                                                  CAUTION !!!\n",
    "#                                         only run on cloud platform !!!\n",
    "\n",
    "'''\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    batch_size = 50\n",
    "    \n",
    "    num_epochs = 25\n",
    "    num_batches = 200*6\n",
    "    \n",
    "    # input for the generator\n",
    "    n = np.random.uniform(0.0, 1.0, [batch_size, 1,1,100]).astype(np.float32)\n",
    "    #batch_iter = batch_generator(folder_path, batch_size)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # refresh the iterator after each epoch\n",
    "        batch_iter = batch_generator(folder_path, batch_size)\n",
    "        new_batch_flag = 1\n",
    "        batch_counter = 0 # counts number of new batches used\n",
    "        \n",
    "        #for i in range(num_batches):\n",
    "        while batch_counter < num_batches:\n",
    "            train_d = True\n",
    "            train_g = True\n",
    "\n",
    "\n",
    "            # will be zero if the previous batch was not used to train the discriminator\n",
    "            # so that the next batch will not be generated (the old batch will be reused)\n",
    "            # problem : the loop will run 1200 times but 1200 new batches wont be used\n",
    "            # possible solution : create a counter(for num of new batches used) \n",
    "            #     and use while loop inplace of the inner for loop\n",
    "\n",
    "            if new_batch_flag == 1: \n",
    "                batch = next(batch_iter) # generate the next batch out of 200*6 batches\n",
    "                batch_counter += 1\n",
    "\n",
    "            d_real_ls, d_fake_ls, g_ls, d_ls = sess.run([dis_loss_real, dis_loss_fake, gen_loss, dis_loss], \n",
    "                                                        feed_dict={input_image: batch, gen_image: n})\n",
    "\n",
    "            d_real_ls = np.mean(d_real_ls)\n",
    "            d_fake_ls = np.mean(d_fake_ls)\n",
    "            g_ls = g_ls\n",
    "            d_ls = d_ls\n",
    "\n",
    "\n",
    "            # the code below will create a problem\n",
    "            # because the next batch will be generated but the \n",
    "            # discriminator will not be trained if train_d = False\n",
    "            # so we are losing some of the training data\n",
    "\n",
    "            # possible solution : generate a batch only \"if train_d = True\"\n",
    "            # or remove the constraints on training\n",
    "            # or create a flag as a feed back for whether or not the batch was used in the previous iteration\n",
    "\n",
    "            if g_ls * 1.5 < d_ls:\n",
    "                train_g = False\n",
    "                pass\n",
    "            if d_ls * 2 < g_ls:\n",
    "                train_d = False\n",
    "                new_batch_flag = 0\n",
    "                pass\n",
    "            if train_d:\n",
    "                sess.run(optimizer_d, feed_dict={gen_image: n, input_image: batch})\n",
    "                new_batch_flag = 1\n",
    "\n",
    "\n",
    "            if train_g:\n",
    "                sess.run(optimizer_g, feed_dict={gen_image: n})\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples():\n",
    "    \n",
    "    # launch a new session and input some random noise into the generator to get an image\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

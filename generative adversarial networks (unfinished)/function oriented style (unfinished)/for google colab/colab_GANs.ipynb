{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = r'C:\\Users\\Admin\\Desktop\\program\\datasets\\cifar-10-batches-py\\data_batch_1'\n",
    "\n",
    "def _unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_parser(file_path, batch_size):\n",
    "    ''' Takes the path to a pickled file as input and returns a numpy\n",
    "        array. 'iter()' can be used to create an iterator and 'next()'\n",
    "        can be used to generate a batch of images everytime.\n",
    "        \n",
    "        Args:\n",
    "            file_path : The full path of the file as a raw string\n",
    "            batch_size : The number of images to generate at each batch\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    d = _unpickle(file_path)\n",
    "    num_images, img_size = d[b'data'].shape[0], d[b'data'].shape[1]\n",
    "    \n",
    "    # batch_data is just the entire data of the pickle file as a numpy array\n",
    "    # which is reshaped in such a way that it is easy to create an iterator\n",
    "    # of the array and generate a batch everytime 'next(batch_size)' is run\n",
    "    batch_data = d[b'data'][:].reshape(num_images//batch_size,batch_size,3,img_size//3).reshape(num_images//batch_size,batch_size,3,int((img_size//3)**0.5),int((img_size//3)**0.5)).transpose((0,1,3,4,2))\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_iterator(folder_path, batch_size):\n",
    "    ''' function that takes the folder path\n",
    "        where all the cifar_10 files are present. 'iter()' can be used \n",
    "        to create an iterator and 'next()' can be used to generate a\n",
    "        batch of images everytime\n",
    "        \n",
    "        Args:\n",
    "            folder_path : The full path of the folder where all the cifar_10\n",
    "                            files are present. raw string input\n",
    "            batch_size : The number of images to generate at each batch\n",
    "    '''\n",
    "    \n",
    "    # make an os.listdir() to get the names of all the files\n",
    "    # get their paths by joining the folder path and the file name\n",
    "    \n",
    "    # The function will yield the 'batch_data' for each file and the 'batch_data'\n",
    "    # can then be iterated \"iter(batch_data)\" during training to get each batch\n",
    "    \n",
    "    # so we are creating a generator from a generator\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        yield cifar_parser(file_path, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_iterator = cifar_iterator(r'C:\\Users\\Admin\\Desktop\\program\\datasets\\cifar-10-batches-py', 50)\n",
    "\n",
    "#num_files = len(os.listdir(r'C:\\Users\\Admin\\Desktop\\program\\datasets\\cifar-10-batches-py'))\n",
    "\n",
    "def batch_generator(folder_path, batch_size):\n",
    "    file_iterator = cifar_iterator(folder_path, batch_size)\n",
    "    num_files = len(os.listdir(folder_path))\n",
    "    \n",
    "    for _ in range(num_files):\n",
    "        file_data = next(file_iterator) # load a file and parse it into (200,50,32,32,3) array\n",
    "        batch_iterator = iter(file_data)\n",
    "        \n",
    "        for _ in range(file_data.shape[0]):\n",
    "            batch_data = next(batch_iterator) # outputs a batch of size (50,32,32,3) from the file of size (200,50,32,32,3)\n",
    "            yield batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_conv2d_transpose(inputs, filters, strides = (2,2), padding = 'same'):\n",
    "    return tf.layers.conv2d_transpose(inputs = inputs,\n",
    "                                        filters = filters, \n",
    "                                        kernel_size = 4, \n",
    "                                        strides = strides, \n",
    "                                        padding = padding, \n",
    "                                        use_bias = False, \n",
    "                                        kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_conv2d(inputs, filters, strides = (2,2), padding = 'same'):\n",
    "    return tf.layers.conv2d(inputs = inputs,\n",
    "                                        filters = filters, \n",
    "                                        kernel_size = 4, \n",
    "                                        strides = strides, \n",
    "                                        padding = padding, \n",
    "                                        use_bias = False, \n",
    "                                        kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_loss(labels_in, logits_in):\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_in, logits=logits_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(g_image, reuse=None):\n",
    "    with tf.variable_scope('g', reuse=reuse):\n",
    "        \n",
    "        # g_image will have the random noise with (1x1x100) dimensions\n",
    "        gnet = helper_conv2d_transpose(g_image, 512, (1,1), 'valid')\n",
    "        gnet = tf.layers.batch_normalization(inputs = gnet)\n",
    "        gnet = tf.nn.relu(gnet)\n",
    "\n",
    "        gnet = helper_conv2d_transpose(gnet, 256)\n",
    "        gnet = tf.layers.batch_normalization(inputs = gnet)\n",
    "        gnet = tf.nn.relu(gnet)\n",
    "                \n",
    "        gnet = helper_conv2d_transpose(gnet, 128, (1,1))\n",
    "        gnet = tf.layers.batch_normalization(inputs = gnet)\n",
    "        gnet = tf.nn.relu(gnet)\n",
    "              \n",
    "        gnet = helper_conv2d_transpose(gnet, 64)\n",
    "        gnet = tf.layers.batch_normalization(inputs = gnet)\n",
    "        gnet = tf.nn.relu(gnet)\n",
    "                      \n",
    "        gnet = helper_conv2d_transpose(gnet, 3)\n",
    "        \n",
    "        gnet = tf.nn.tanh(gnet)\n",
    "        \n",
    "        return gnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x_image, reuse=None):\n",
    "    with tf.variable_scope('d', reuse=reuse):\n",
    "        \n",
    "        # x_image will take true images and generator images alternatively\n",
    "        # same weights will be used(reuse=True) to train both fake and real images\n",
    "        dnet = helper_conv2d(x_image, 64)\n",
    "        dnet = tf.nn.leaky_relu(dnet)                        \n",
    "    \n",
    "        dnet = helper_conv2d(dnet, 128)\n",
    "        dnet = tf.layers.batch_normalization(inputs = dnet)\n",
    "        dnet = tf.nn.leaky_relu(dnet)\n",
    "\n",
    "        dnet = helper_conv2d(dnet, 256)\n",
    "        dnet = tf.layers.batch_normalization(inputs = dnet)\n",
    "        dnet = tf.nn.leaky_relu(dnet)\n",
    "             \n",
    "        dnet = helper_conv2d(dnet, 512)\n",
    "        dnet = tf.layers.batch_normalization(inputs = dnet)\n",
    "        dnet = tf.nn.leaky_relu(dnet)    \n",
    "        \n",
    "        #dnet = helper_conv2d(dnet, 1, (1,1), 'valid')\n",
    "        \n",
    "        dnet = tf.layers.flatten(dnet)\n",
    "        \n",
    "        dnet = tf.layers.dense(dnet, 1024, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        dnet = tf.layers.dropout(dnet, 0.5)\n",
    "        \n",
    "        dnet = tf.layers.dense(dnet, 128, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        dnet = tf.layers.dropout(dnet, 0.5)\n",
    "        \n",
    "        dnet = tf.layers.dense(dnet, 1, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        out = tf.nn.sigmoid(dnet)\n",
    "           \n",
    "        return out, dnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if fetching data from google drive then change the address accordingly\n",
    "\n",
    "folder_path = r'C:\\Users\\Admin\\Desktop\\program\\datasets\\cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for the input images(discriminator) and noise(generator)\n",
    "\n",
    "input_image = tf.placeholder(tf.float32, [None, 32,32,3]) # feed input images\n",
    "gen_image = tf.placeholder(tf.float32, [None, 1,1,100]) # feed random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables for generator and discriminator\n",
    "\n",
    "gen = generator(gen_image)\n",
    "dis_output_real, dis_logits_real = discriminator(input_image)\n",
    "dis_output_fake, dis_logits_fake = discriminator(gen, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the trainable variables to apply regularization\n",
    "\n",
    "vars_g = [var for var in tf.trainable_variables() if var.name.startswith('g')]\n",
    "vars_d = [var for var in tf.trainable_variables() if var.name.startswith('d')]\n",
    "\n",
    "reg_g = tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(1e-6), vars_g)\n",
    "reg_d = tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(1e-6), vars_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_loss_real = helper_loss(tf.ones_like(dis_logits_real), dis_logits_real)\n",
    "dis_loss_fake = helper_loss(tf.zeros_like(dis_logits_fake), dis_logits_fake)\n",
    "\n",
    "dis_loss = tf.reduce_mean((dis_loss_real + dis_loss_fake)*0.5)\n",
    "gen_loss = tf.reduce_mean(helper_loss(tf.ones_like(dis_logits_fake), dis_logits_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer_dis = tf.train.AdamOptimizer(learning_rate=0.0002).minimize(dis_loss + reg_d, var_list = vars_d)\n",
    "    optimizer_gen = tf.train.AdamOptimizer(learning_rate=0.0002).minimize(dis_loss + reg_d, var_list = vars_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith tf.Session() as sess:\\n    \\n    batch_size = 50\\n    \\n    num_epochs = 25\\n    num_batches = 200*6\\n    \\n    # input for the generator\\n    n = np.random.uniform(0.0, 1.0, [batch_size, 1,1,100]).astype(np.float32)\\n    #batch_iter = batch_generator(folder_path, batch_size)\\n    \\n    for epoch in range(num_epochs):\\n        \\n        # refresh the iterator after each epoch\\n        batch_iter = batch_generator(folder_path, batch_size)\\n        new_batch_flag = 1\\n        batch_counter = 0 # counts number of new batches used\\n        \\n            #for i in range(num_batches):\\n            while batch_counter < num_batches:\\n                train_d = True\\n                train_g = True\\n                \\n                \\n                # will be zero if the previous batch was not used to train the discriminator\\n                # so that the next batch will not be generated (the old batch will be reused)\\n                # problem : the loop will run 1200 times but 1200 new batches wont be used\\n                # possible solution : create a counter(for num of new batches used) \\n                #     and use while loop inplace of the inner for loop\\n                \\n                if new_batch_flag == 1: \\n                    batch = next(batch_iter) # generate the next batch out of 200*6 batches\\n                    batch_counter += 1\\n                    \\n                d_real_ls, d_fake_ls, g_ls, d_ls = sess.run([dis_loss_real, dis_loss_fake, gen_loss, dis_loss], \\n                                                            feed_dict={input_image: batch, gen_image: n})\\n                \\n                d_real_ls = np.mean(d_real_ls)\\n                d_fake_ls = np.mean(d_fake_ls)\\n                g_ls = g_ls\\n                d_ls = d_ls\\n                \\n                \\n                # the code below will create a problem\\n                # because the next batch will be generated but the \\n                # discriminator will not be trained if train_d = False\\n                # so we are losing some of the training data\\n                \\n                # possible solution : generate a batch only \"if train_d = True\"\\n                # or remove the constraints on training\\n                # or create a flag as a feed back for whether or not the batch was used in the previous iteration\\n                \\n                if g_ls * 1.5 < d_ls:\\n                    train_g = False\\n                    pass\\n                if d_ls * 2 < g_ls:\\n                    train_d = False\\n                    new_batch_flag = 0\\n                    pass\\n                if train_d:\\n                    sess.run(optimizer_d, feed_dict={gen_image: n, input_image: batch})\\n                    new_batch_flag = 1\\n\\n\\n                if train_g:\\n                    sess.run(optimizer_g, feed_dict={gen_image: n})\\n                \\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#                                                  CAUTION !!!\n",
    "#                                         only run on cloud platform !!!\n",
    "\n",
    "'''\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    batch_size = 50\n",
    "    \n",
    "    num_epochs = 25\n",
    "    num_batches = 200*6\n",
    "    \n",
    "    # input for the generator\n",
    "    n = np.random.uniform(0.0, 1.0, [batch_size, 1,1,100]).astype(np.float32)\n",
    "    #batch_iter = batch_generator(folder_path, batch_size)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # refresh the iterator after each epoch\n",
    "        batch_iter = batch_generator(folder_path, batch_size)\n",
    "        new_batch_flag = 1\n",
    "        batch_counter = 0 # counts number of new batches used\n",
    "        \n",
    "            #for i in range(num_batches):\n",
    "            while batch_counter < num_batches:\n",
    "                train_d = True\n",
    "                train_g = True\n",
    "                \n",
    "                \n",
    "                # will be zero if the previous batch was not used to train the discriminator\n",
    "                # so that the next batch will not be generated (the old batch will be reused)\n",
    "                # problem : the loop will run 1200 times but 1200 new batches wont be used\n",
    "                # possible solution : create a counter(for num of new batches used) \n",
    "                #     and use while loop inplace of the inner for loop\n",
    "                \n",
    "                if new_batch_flag == 1: \n",
    "                    batch = next(batch_iter) # generate the next batch out of 200*6 batches\n",
    "                    batch_counter += 1\n",
    "                    \n",
    "                d_real_ls, d_fake_ls, g_ls, d_ls = sess.run([dis_loss_real, dis_loss_fake, gen_loss, dis_loss], \n",
    "                                                            feed_dict={input_image: batch, gen_image: n})\n",
    "                \n",
    "                d_real_ls = np.mean(d_real_ls)\n",
    "                d_fake_ls = np.mean(d_fake_ls)\n",
    "                g_ls = g_ls\n",
    "                d_ls = d_ls\n",
    "                \n",
    "                \n",
    "                # the code below will create a problem\n",
    "                # because the next batch will be generated but the \n",
    "                # discriminator will not be trained if train_d = False\n",
    "                # so we are losing some of the training data\n",
    "                \n",
    "                # possible solution : generate a batch only \"if train_d = True\"\n",
    "                # or remove the constraints on training\n",
    "                # or create a flag as a feed back for whether or not the batch was used in the previous iteration\n",
    "                \n",
    "                if g_ls * 1.5 < d_ls:\n",
    "                    train_g = False\n",
    "                    pass\n",
    "                if d_ls * 2 < g_ls:\n",
    "                    train_d = False\n",
    "                    new_batch_flag = 0\n",
    "                    pass\n",
    "                if train_d:\n",
    "                    sess.run(optimizer_d, feed_dict={gen_image: n, input_image: batch})\n",
    "                    new_batch_flag = 1\n",
    "\n",
    "\n",
    "                if train_g:\n",
    "                    sess.run(optimizer_g, feed_dict={gen_image: n})\n",
    "                \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples():\n",
    "    \n",
    "    # launch a new session and input some random noise into the generator to get an image\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
